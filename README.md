# ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with Multi-dimensional and fine-grained information
This directory contains the ChineseWebText2.0 dataset, and a new tool-chain called MDFG-tool for constructing large-scale and high-quality Chinese datasets with multi-dimensional and fine-grained information. Our ChineseWebText2.0 dataset is publicly available on huggingface (here).
## ChineseWebText2.0
- ### Dataset Overview
We have released the latest and largest Chinese dataset, ChineseWebText 2.0, which consists of 3.8 TB of data. Each text in the dataset is accompanied by a quality score, domain single-label and multi-label tags, as well as toxicity classification and scores, enabling LLM researchers to select data based on new quality thresholds.
- ### Data Example
   ```json
  {
   "text": "近日，黑龙江省高校校报协会第十四届学术年会暨校报工作交流研讨会在东北农业大学举行。我校10件新闻作品喜获2项一等奖，2项二等奖，6项三等奖……",
   "domain":
           {
             "single_label": "news",
             "multi_label": ["news", "education"]
           },
   "toxicity":
           {
             "label": 0,
             "score": 1.0347155694034882e-05
           },
   "quality_score": 0.96044921875
   }
    ```
   
- "text": 【string】Text content of data sample.
- "single_label": 【string】The highest probability label generated by the domain classification model.
- "multi_label": 【list】All labels generated by the domain classification model with probabilities higher than the threshold.
- "label": 【int】Toxicity label generated by toxicity classification models.
- "score": 【flaot】Toxicity score generated by toxicity classification model.
- "quality_score": 【float】Quality score generated by the quality evaluation model.

## MDFG-tool

### Introduction

We introduce a new toolchain, MDFG-tool (see Figure 1). We begin with the coarse-grained filtering module, which applies rule-based methods to clean the data, focusing on criteria such as text length and sensitive words to ensure data quality. After cleaning, we evaluate the text quality using a BERT-based model. This process generates a quality score, and by selecting an appropriate threshold, we can extract high-quality text data that meets our needs. Next, we use FastText for both single-label and multi-label classification of the cleaned data. Meanwhile, we conduct toxicity assessment. The FastText model is used to filter out toxic content and assign toxicity scores to each text. This scoring system allows researchers to set thresholds for identifying and selecting harmful texts for further training.

<div align="center">
  <img src=".\assets\structure.png" width="50%" />
</div>

### Environment Dependencies

transformers==4.31.0
scipy==1.11.1
numpy==1.24.3
jieba==0.42.1
zhconv==1.4.3
fasttext==0.9.2
